{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grapheme-to-Phoneme Conversion with LSTM in Pytorch\n",
    "\n",
    "In this blog post, we will explore a complete implementation of a Grapheme-to-Phoneme (G2P) conversion model using LSTM architecture in PyTorch. This model uses a LSTM based seq-to-seq encoder-decoder architecture with attention mechanism to convert graphemes (written characters) into phonemes (sounds). We'll go through each part of the code in detail, explaining the purpose and functionality of each section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing Libraries and Downloading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to /home/pdeb/nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import nltk\n",
    "from nltk.corpus import cmudict\n",
    "import random\n",
    "\n",
    "nltk.download('cmudict')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We download the CMU Pronouncing Dictionary, which contains mappings from words to their phonetic transcriptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Loading and Preparing Data\n",
    "\n",
    "We create a list `data` where each element is a tuple containing a word and its phonetic transcription. We limit the dataset to the first 200 entries for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n"
     ]
    }
   ],
   "source": [
    "# Load CMUdict\n",
    "cmu = cmudict.dict()\n",
    "\n",
    "# Prepare data\n",
    "data = []\n",
    "for word, pronunciations in cmu.items():\n",
    "    for phones in pronunciations:\n",
    "        data.append((word, ' '.join(phones)))\n",
    "\n",
    "data = data[:20000]\n",
    "print(len(data))\n",
    "\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building Vocabularies\n",
    "\n",
    "We create sets of unique graphemes and phonemes from the data. We also add special tokens (`<pad>`, `<sos>`, `<eos>`) to the vocabularies. Furthermore, we create mappings from characters/phonemes to indices and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabularies\n",
    "grapheme_vocab = set(''.join(word for word, _ in data))\n",
    "phoneme_vocab = set(' '.join(phones for _, phones in data).split())\n",
    "\n",
    "grapheme_vocab = ['<pad>', '<sos>', '<eos>'] + list(grapheme_vocab)\n",
    "phoneme_vocab = ['<pad>', '<sos>', '<eos>'] + list(phoneme_vocab)\n",
    "\n",
    "grapheme_vocab_size = len(grapheme_vocab)\n",
    "phoneme_vocab_size = len(phoneme_vocab)\n",
    "\n",
    "grapheme2idx = {char: idx for idx, char in enumerate(grapheme_vocab)}\n",
    "phoneme2idx = {phone: idx for idx, phone in enumerate(phoneme_vocab)}\n",
    "\n",
    "idx2grapheme = {idx: char for idx, char in enumerate(grapheme_vocab)}\n",
    "idx2phoneme = {idx: phone for idx, phone in enumerate(phoneme_vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creating the Dataset and DataLoader\n",
    "\n",
    "We define a custom dataset class `G2PDataset` that takes the data, grapheme2idx, and phoneme2idx as inputs. The `__getitem__` method converts the word and phonetic transcription into indices, and the `collate_fn` function pads the sequences in the batch to the maximum length in the batch.\n",
    "\n",
    "we have also added a small test to check if the dataloader is working fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1:\n",
      "Grapheme indices: tensor([[ 1, 21, 34, 27, 39, 33, 39, 34,  2,  0,  0,  0,  0,  0,  0],\n",
      "        [ 1, 21,  7,  7, 39, 30,  3, 19, 11, 30, 21,  5,  2,  0,  0],\n",
      "        [ 1, 26, 21,  5,  7, 19, 39, 33,  2,  0,  0,  0,  0,  0,  0],\n",
      "        [ 1, 21, 26, 39, 38, 11, 19, 26, 27, 16, 19, 15,  5, 39,  2],\n",
      "        [ 1,  3, 10, 38, 38, 21, 33,  7,  2,  0,  0,  0,  0,  0,  0],\n",
      "        [ 1, 21,  3,  7, 38, 33, 10,  7, 39,  2,  0,  0,  0,  0,  0],\n",
      "        [ 1,  3, 21, 33, 33, 15,  7,  2,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 1,  3, 10, 26, 31, 39, 38,  7,  2,  0,  0,  0,  0,  0,  0],\n",
      "        [ 1, 21,  5, 38, 21,  8, 16,  5, 15,  7, 30,  7,  2,  0,  0],\n",
      "        [ 1,  3, 16, 19, 15, 38, 27, 16,  2,  0,  0,  0,  0,  0,  0],\n",
      "        [ 1,  3, 19, 10, 33,  7,  2,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 1,  3, 39,  7,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 1, 21, 33, 21, 33, 21, 38,  2,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 1, 21,  3, 10, 34, 33, 21, 27, 30,  2,  0,  0,  0,  0,  0],\n",
      "        [ 1,  3, 33, 39, 15, 38, 19, 15,  5,  8,  2,  0,  0,  0,  0],\n",
      "        [ 1, 26, 21, 19, 15, 13, 13,  2,  0,  0,  0,  0,  0,  0,  0]])\n",
      "Phoneme indices: tensor([[ 1, 52, 70, 34, 62, 50, 70,  2,  0,  0,  0,  0,  0,  0],\n",
      "        [ 1, 45, 46,  3, 19, 31, 63, 40, 19, 16, 53,  2,  0,  0],\n",
      "        [ 1, 38, 30, 53, 46, 63, 39,  2,  0,  0,  0,  0,  0,  0],\n",
      "        [ 1, 45, 46, 26, 49, 45, 63, 38, 11, 63, 40, 53,  2,  0],\n",
      "        [ 1, 31, 28, 49, 39, 10,  2,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 1, 45, 31, 46, 49, 50,  8, 46,  2,  0,  0,  0,  0,  0],\n",
      "        [ 1, 31,  3, 50, 20, 46,  2,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 1, 31, 28, 38, 45, 49, 46,  2,  0,  0,  0,  0,  0,  0],\n",
      "        [ 1, 52, 53, 49, 30, 17, 45, 53, 44, 10, 45, 19, 10,  2],\n",
      "        [ 1, 31, 14, 63, 62, 37, 45,  2,  0,  0,  0,  0,  0,  0],\n",
      "        [ 1, 31, 63, 69, 10,  2,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 1, 31, 36, 10,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 1, 30, 50, 45, 50, 43, 49,  2,  0,  0,  0,  0,  0,  0],\n",
      "        [ 1, 45, 31, 28, 70, 50, 45, 19,  2,  0,  0,  0,  0,  0],\n",
      "        [ 1, 31, 50, 42, 49, 63, 20, 15,  2,  0,  0,  0,  0,  0],\n",
      "        [ 1, 38, 30, 63, 20,  7,  2,  0,  0,  0,  0,  0,  0,  0]])\n",
      "Batch 2:\n",
      "Grapheme indices: tensor([[ 1,  3, 33, 16, 24, 16, 12,  7, 31, 15,  2],\n",
      "        [ 1,  3, 16, 13, 16, 33,  7, 23,  7,  2,  0],\n",
      "        [ 1, 26, 21, 19, 38, 39, 14,  2,  0,  0,  0],\n",
      "        [ 1,  3, 19, 10,  5, 26, 31,  2,  0,  0,  0],\n",
      "        [ 1,  3, 33, 21, 10, 34,  2,  0,  0,  0,  0],\n",
      "        [ 1, 21,  3, 21, 38, 39,  7,  2,  0,  0,  0],\n",
      "        [ 1, 26, 21, 11, 12, 16, 16, 34,  2,  0,  0],\n",
      "        [ 1,  3, 16, 24, 39, 19, 19, 23,  7,  2,  0],\n",
      "        [ 1, 21, 30, 21,  7,  7,  2,  0,  0,  0,  0],\n",
      "        [ 1,  3, 16, 16, 31, 16, 10, 38,  2,  0,  0],\n",
      "        [ 1,  3, 33, 21, 34, 39, 30, 21,  7,  2,  0],\n",
      "        [ 1,  3, 39, 27, 16, 19, 34, 15,  5,  8,  2],\n",
      "        [ 1,  3, 39, 27, 19,  2,  0,  0,  0,  0,  0],\n",
      "        [ 1,  3, 10, 38, 38, 15,  5,  8,  2,  0,  0],\n",
      "        [ 1,  3, 10, 33, 31, 27, 21, 33, 34,  2,  0],\n",
      "        [ 1, 21, 39,  8, 39, 21,  5,  2,  0,  0,  0]])\n",
      "Phoneme indices: tensor([[ 1, 31, 50, 45, 10, 59,  7, 46, 38, 40,  2],\n",
      "        [ 1, 31, 11,  7, 39, 10, 20, 10,  2,  0,  0],\n",
      "        [ 1, 38, 28, 63, 49,  3, 38, 46,  2,  0,  0],\n",
      "        [ 1, 31, 63, 28, 15, 38,  2,  0,  0,  0,  0],\n",
      "        [ 1, 31, 50, 59, 70,  2,  0,  0,  0,  0,  0],\n",
      "        [ 1, 45, 31, 51, 49, 46,  2,  0,  0,  0,  0],\n",
      "        [ 1, 38, 51, 29, 66, 70,  2,  0,  0,  0,  0],\n",
      "        [ 1, 31, 14, 10,  3, 63, 10,  2,  0,  0,  0],\n",
      "        [ 1, 45, 19, 30, 46,  2,  0,  0,  0,  0,  0],\n",
      "        [ 1, 31, 68, 38, 13, 49,  2,  0,  0,  0,  0],\n",
      "        [ 1, 31, 50, 45, 70, 51, 19, 45, 46,  2,  0],\n",
      "        [ 1, 31, 20, 34, 11, 63, 70, 20, 15,  2,  0],\n",
      "        [ 1, 31,  3, 63,  2,  0,  0,  0,  0,  0,  0],\n",
      "        [ 1, 31, 28, 49, 20, 15,  2,  0,  0,  0,  0],\n",
      "        [ 1, 31, 69, 38, 34, 39, 70,  2,  0,  0,  0],\n",
      "        [ 1, 20, 55, 36, 45, 53,  2,  0,  0,  0,  0]])\n",
      "\n",
      "Grapheme vocabulary size: 42\n",
      "Phoneme vocabulary size: 72\n"
     ]
    }
   ],
   "source": [
    "class G2PDataset(Dataset):\n",
    "    \"\"\"A custom Dataset for Grapheme-to-Phoneme conversion.\n",
    "\n",
    "    This dataset prepares word-pronunciation pairs for G2P model training.\n",
    "\n",
    "    Args:\n",
    "        data (List[Tuple[str, str]]): List of (word, pronunciation) pairs.\n",
    "        grapheme2idx (Dict[str, int]): Mapping from graphemes to indices.\n",
    "        phoneme2idx (Dict[str, int]): Mapping from phonemes to indices.\n",
    "\n",
    "    Attributes:\n",
    "        data (List[Tuple[str, str]]): The input data.\n",
    "        grapheme2idx (Dict[str, int]): Grapheme to index mapping.\n",
    "        phoneme2idx (Dict[str, int]): Phoneme to index mapping.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, grapheme2idx, phoneme2idx):\n",
    "        self.data = data\n",
    "        self.grapheme2idx = grapheme2idx\n",
    "        self.phoneme2idx = phoneme2idx\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the number of items in the dataset.\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Generates one sample of data.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the sample to fetch.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor]: A tuple containing:\n",
    "                - Tensor of grapheme indices.\n",
    "                - Tensor of phoneme indices.\n",
    "        \"\"\"\n",
    "        word, phones = self.data[idx]\n",
    "        grapheme_indices = [self.grapheme2idx['<sos>']] + [self.grapheme2idx[char] for char in word] + [self.grapheme2idx['<eos>']]\n",
    "        phoneme_indices = [self.phoneme2idx['<sos>']] + [self.phoneme2idx[phone] for phone in phones.split()] + [self.phoneme2idx['<eos>']]\n",
    "        return torch.tensor(grapheme_indices, dtype=torch.long), torch.tensor(phoneme_indices, dtype=torch.long)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Collate function for DataLoader.\n",
    "\n",
    "    This function pads sequences in a batch to the maximum length.\n",
    "\n",
    "    Args:\n",
    "        batch (List[Tuple[torch.Tensor, torch.Tensor]]): A list of tuples containing grapheme and phoneme tensors.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[torch.Tensor, torch.Tensor]: A tuple containing:\n",
    "            - Padded grapheme tensor of shape (batch_size, max_grapheme_len).\n",
    "            - Padded phoneme tensor of shape (batch_size, max_phoneme_len).\n",
    "    \"\"\"\n",
    "    grapheme_batch, phoneme_batch = zip(*batch)\n",
    "    \n",
    "    # Find the maximum sequence length in the batch\n",
    "    grapheme_max_len = max(len(grapheme) for grapheme in grapheme_batch)\n",
    "    phoneme_max_len = max(len(phoneme) for phoneme in phoneme_batch)\n",
    "    \n",
    "    # Pad sequences in the batch\n",
    "    grapheme_padded = [torch.cat([grapheme, torch.full((grapheme_max_len - len(grapheme),), grapheme2idx['<pad>'], dtype=torch.long)]) for grapheme in grapheme_batch]\n",
    "    phoneme_padded = [torch.cat([phoneme, torch.full((phoneme_max_len - len(phoneme),), phoneme2idx['<pad>'], dtype=torch.long)]) for phoneme in phoneme_batch]\n",
    "\n",
    "    return torch.stack(grapheme_padded), torch.stack(phoneme_padded)\n",
    "\n",
    "# Create dataset and dataloader with custom collate_fn\n",
    "dataset = G2PDataset(data, grapheme2idx, phoneme2idx)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Test dataloader\n",
    "for batch_idx, (graphemes, phonemes) in enumerate(dataloader):\n",
    "    print(f\"Batch {batch_idx + 1}:\")\n",
    "    print(\"Grapheme indices:\", graphemes)\n",
    "    print(\"Phoneme indices:\", phonemes)\n",
    "    if batch_idx == 1:  # Print only first two batches\n",
    "        break\n",
    "\n",
    "# Print vocabulary sizes\n",
    "print(f\"\\nGrapheme vocabulary size: {grapheme_vocab_size}\")\n",
    "print(f\"Phoneme vocabulary size: {phoneme_vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Defining the Model\n",
    "\n",
    "We define the encoder, attention mechanism, decoder, and the entire G2P model. We also set the hyperparameters for the model.\n",
    "\n",
    "The `encoder` is a simple LSTM that takes the input grapheme indices and returns the output features, final hidden state, and cell state.\n",
    "\n",
    "The `attention mechanism` calculates the attention weights based on the current hidden state of the decoder and all hidden states of the encoder.\n",
    "\n",
    "The `decoder` is another LSTM that takes the input phoneme index, current hidden state, cell state, and all hidden states of the encoder. It returns the output prediction, updated hidden state, cell state, and attention weights.\n",
    "\n",
    "Finally, the `G2P model` combines the `encoder`, `decoder`, and `attention mechanism` to produce the output predictions and attention weights.\n",
    "\n",
    "We set the hyperparameters for the model, including the input and output sizes, embedding size, hidden size, number of layers, and dropout rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"Encoder module for the Grapheme-to-Phoneme model.\n",
    "\n",
    "    Args:\n",
    "        input_size (int): Size of the input vocabulary.\n",
    "        embedding_size (int): Size of the embedding layer.\n",
    "        hidden_size (int): Number of features in the hidden state.\n",
    "        num_layers (int): Number of recurrent layers.\n",
    "        dropout (float): Dropout rate.\n",
    "\n",
    "    Attributes:\n",
    "        hidden_size (int): Number of features in the hidden state.\n",
    "        num_layers (int): Number of recurrent layers.\n",
    "        embedding (nn.Embedding): Embedding layer.\n",
    "        lstm (nn.LSTM): LSTM layer.\n",
    "        dropout (nn.Dropout): Dropout layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass of the encoder.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, seq_len).\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing:\n",
    "                - outputs (torch.Tensor): Output features from the LSTM.\n",
    "                - hidden (torch.Tensor): Final hidden state of the LSTM.\n",
    "                - cell (torch.Tensor): Final cell state of the LSTM.\n",
    "        \"\"\"\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        return outputs, hidden, cell\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"Attention mechanism for the Grapheme-to-Phoneme model.\n",
    "\n",
    "    Args:\n",
    "        hidden_size (int): Number of features in the hidden state.\n",
    "\n",
    "    Attributes:\n",
    "        hidden_size (int): Number of features in the hidden state.\n",
    "        attn (nn.Linear): Linear layer for attention calculation.\n",
    "        v (nn.Linear): Linear layer for attention weights.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.v = nn.Linear(hidden_size, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        \"\"\"Forward pass of the attention mechanism.\n",
    "\n",
    "        Args:\n",
    "            hidden (torch.Tensor): Current hidden state of the decoder.\n",
    "            encoder_outputs (torch.Tensor): All hidden states of the encoder.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Attention weights.\n",
    "        \"\"\"\n",
    "        batch_size = encoder_outputs.shape[0]\n",
    "        seq_len = encoder_outputs.shape[1]\n",
    "\n",
    "        hidden = hidden.repeat(seq_len, 1, 1).transpose(0, 1)\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        return torch.softmax(attention, dim=1)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"Decoder module for the Grapheme-to-Phoneme model.\n",
    "\n",
    "    Args:\n",
    "        output_size (int): Size of the output vocabulary.\n",
    "        embedding_size (int): Size of the embedding layer.\n",
    "        hidden_size (int): Number of features in the hidden state.\n",
    "        num_layers (int): Number of recurrent layers.\n",
    "        dropout (float): Dropout rate.\n",
    "\n",
    "    Attributes:\n",
    "        output_size (int): Size of the output vocabulary.\n",
    "        hidden_size (int): Number of features in the hidden state.\n",
    "        num_layers (int): Number of recurrent layers.\n",
    "        embedding (nn.Embedding): Embedding layer.\n",
    "        lstm (nn.LSTM): LSTM layer.\n",
    "        fc (nn.Linear): Fully connected layer for output prediction.\n",
    "        dropout (nn.Dropout): Dropout layer.\n",
    "        attention (Attention): Attention mechanism.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size, embedding_size, hidden_size, num_layers, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(hidden_size + embedding_size, hidden_size, num_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attention = Attention(hidden_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell, encoder_outputs):\n",
    "        \"\"\"Forward pass of the decoder.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor (current target token).\n",
    "            hidden (torch.Tensor): Current hidden state.\n",
    "            cell (torch.Tensor): Current cell state.\n",
    "            encoder_outputs (torch.Tensor): All hidden states of the encoder.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing:\n",
    "                - prediction (torch.Tensor): Output prediction.\n",
    "                - hidden (torch.Tensor): Updated hidden state.\n",
    "                - cell (torch.Tensor): Updated cell state.\n",
    "                - attention_weights (torch.Tensor): Attention weights.\n",
    "        \"\"\"\n",
    "        x = x.unsqueeze(1)\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "\n",
    "        attention_weights = self.attention(hidden[-1], encoder_outputs)\n",
    "        context_vector = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)\n",
    "\n",
    "        lstm_input = torch.cat((embedded, context_vector), dim=2)\n",
    "        output, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))\n",
    "\n",
    "        prediction = self.fc(torch.cat((output.squeeze(1), context_vector.squeeze(1)), dim=1))\n",
    "        return prediction, hidden, cell, attention_weights\n",
    "\n",
    "class G2P(nn.Module):\n",
    "    \"\"\"Grapheme-to-Phoneme (G2P) model.\n",
    "\n",
    "    Args:\n",
    "        encoder (Encoder): Encoder module.\n",
    "        decoder (Decoder): Decoder module.\n",
    "        device (torch.device): Device to run the model on.\n",
    "\n",
    "    Attributes:\n",
    "        encoder (Encoder): Encoder module.\n",
    "        decoder (Decoder): Decoder module.\n",
    "        device (torch.device): Device to run the model on.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(G2P, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, source, target, teacher_force_ratio=0.5):\n",
    "        \"\"\"Forward pass of the G2P model.\n",
    "\n",
    "        Args:\n",
    "            source (torch.Tensor): Source sequence (graphemes).\n",
    "            target (torch.Tensor): Target sequence (phonemes).\n",
    "            teacher_force_ratio (float, optional): Probability of using teacher forcing. Defaults to 0.5.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing:\n",
    "                - outputs (torch.Tensor): Output predictions for each time step.\n",
    "                - attention_weights (torch.Tensor): Attention weights for each time step.\n",
    "        \"\"\"\n",
    "        batch_size = source.shape[0]\n",
    "        target_len = target.shape[1]\n",
    "        target_vocab_size = self.decoder.output_size\n",
    "\n",
    "        outputs = torch.zeros(batch_size, target_len, target_vocab_size).to(self.device)\n",
    "        attention_weights = torch.zeros(batch_size, target_len, source.shape[1]).to(self.device)\n",
    "\n",
    "        encoder_outputs, hidden, cell = self.encoder(source)\n",
    "\n",
    "        x = target[:, 0]\n",
    "        for t in range(1, target_len):\n",
    "            output, hidden, cell, attn_weights = self.decoder(x, hidden, cell, encoder_outputs)\n",
    "            outputs[:, t] = output\n",
    "            attention_weights[:, t] = attn_weights\n",
    "\n",
    "            teacher_force = random.random() < teacher_force_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            x = target[:, t] if teacher_force else top1\n",
    "\n",
    "        return outputs, attention_weights\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = len(grapheme_vocab)\n",
    "output_size = len(phoneme_vocab)\n",
    "embedding_size = 128\n",
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "dropout = 0.2\n",
    "learning_rate = 0.001\n",
    "num_epochs = 30\n",
    "\n",
    "# Initialize model\n",
    "encoder = Encoder(input_size, embedding_size, hidden_size, num_layers, dropout)\n",
    "decoder = Decoder(output_size, embedding_size, hidden_size, num_layers, dropout)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = G2P(encoder, decoder, device).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=phoneme2idx['<pad>'])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training the Model\n",
    "\n",
    "We define the training loop where we iterate over the dataset for a specified number of epochs defined by `num_epochs` in the hyperparamters defined above. \n",
    "For each epoch, we calculate the loss for each batch and update the model parameters using the optimizer.\n",
    "\n",
    "We also clip the gradients to prevent exploding gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:49<00:00, 25.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Loss: 0.4820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:47<00:00, 26.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/30], Loss: 0.4195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:50<00:00, 24.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/30], Loss: 0.3719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:50<00:00, 24.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/30], Loss: 0.3356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:49<00:00, 25.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/30], Loss: 0.3027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:47<00:00, 26.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/30], Loss: 0.2771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:46<00:00, 26.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/30], Loss: 0.2541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:48<00:00, 25.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/30], Loss: 0.2356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:39<00:00, 31.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/30], Loss: 0.2171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:42<00:00, 29.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/30], Loss: 0.2053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:44<00:00, 28.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/30], Loss: 0.1902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:51<00:00, 24.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/30], Loss: 0.1747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:42<00:00, 29.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/30], Loss: 0.1728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:44<00:00, 27.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/30], Loss: 0.1631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:43<00:00, 28.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/30], Loss: 0.1582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:45<00:00, 27.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/30], Loss: 0.1482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:46<00:00, 27.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/30], Loss: 0.1435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:50<00:00, 24.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/30], Loss: 0.1408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:46<00:00, 26.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/30], Loss: 0.1328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:42<00:00, 29.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/30], Loss: 0.1289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:46<00:00, 26.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/30], Loss: 0.1323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:49<00:00, 25.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/30], Loss: 0.1219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:49<00:00, 25.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/30], Loss: 0.1207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:48<00:00, 25.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/30], Loss: 0.1182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:46<00:00, 26.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/30], Loss: 0.1215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:46<00:00, 27.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/30], Loss: 0.1145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:48<00:00, 26.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/30], Loss: 0.1128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:46<00:00, 26.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/30], Loss: 0.1110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:50<00:00, 24.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/30], Loss: 0.1104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:46<00:00, 26.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/30], Loss: 0.1109\n",
      "Training completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, (src, tgt) in enumerate(tqdm(dataloader)):\n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output, _ = model(src, tgt)\n",
    "\n",
    "        output = output[:, 1:].reshape(-1, output.shape[2])\n",
    "        tgt = tgt[:, 1:].reshape(-1)\n",
    "\n",
    "        loss = criterion(output, tgt)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Testing the Model and Visualizing Attention\n",
    "\n",
    "We define a module `g2p_convert` to convert a word into its phonetic transcription using the trained model. \n",
    "This function takes the model, the word, and the mappings between indices and graphemes/phonemes. It returns the predicted phonemes and the attention weights.\n",
    "\n",
    "We also add a module `plot_attention` to visualize the attention weights. This helps us understand the attention mechanism and how it focuses on different parts of the input sequence when generating the output sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uqq seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input word: archivist\n",
      "Phonemes: AA1 R K AH0 V IH0 S T\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def g2p_convert(model, word, grapheme2idx, idx2phoneme, phoneme2idx, device, max_length=50):\n",
    "    model.eval()\n",
    "    \n",
    "    graphemes = ['<sos>'] + list(word.lower()) + ['<eos>']\n",
    "    grapheme_indices = [grapheme2idx.get(char, grapheme2idx['<sos>']) for char in graphemes]\n",
    "    grapheme_tensor = torch.LongTensor(grapheme_indices).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs, hidden, cell = model.encoder(grapheme_tensor)\n",
    "\n",
    "    outputs = []\n",
    "    attentions = []\n",
    "    x = torch.tensor([phoneme2idx['<sos>']]).to(device)\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        output, hidden, cell, attn_weights = model.decoder(x, hidden, cell, encoder_outputs)\n",
    "        \n",
    "        predicted = output.argmax(1)\n",
    "        \n",
    "        if predicted.item() == phoneme2idx['<eos>']:\n",
    "            break\n",
    "\n",
    "        outputs.append(predicted.item())\n",
    "        attentions.append(attn_weights.squeeze(0).cpu().detach().numpy())\n",
    "        x = predicted\n",
    "\n",
    "    phonemes = [idx2phoneme[idx] for idx in outputs]\n",
    "    return ' '.join(phonemes), np.array(attentions)\n",
    "\n",
    "def plot_attention(word, phonemes, attention):\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "    # Prepare input and output tokens\n",
    "    input_tokens = list(word.lower())\n",
    "    output_tokens = phonemes.split()\n",
    "\n",
    "    # Ensure attention has the correct shape (target_length x source_length)\n",
    "    attention = attention[:len(output_tokens), :len(input_tokens)]\n",
    "\n",
    "    # Plot attention heatmap\n",
    "    sns.heatmap(attention, cmap='YlGnBu', ax=ax, cbar=False)\n",
    "\n",
    "    # Manually set the tick positions to match the tokens\n",
    "    ax.set_xticks(np.arange(len(input_tokens)))\n",
    "    ax.set_yticks(np.arange(len(output_tokens)))\n",
    "\n",
    "    # Set the tick labels to the tokens\n",
    "    ax.set_xticklabels(input_tokens, rotation=90)\n",
    "    ax.set_yticklabels(output_tokens, rotation=0)\n",
    "\n",
    "    ax.set_xlabel('Graphemes')\n",
    "    ax.set_ylabel('Phonemes')\n",
    "    ax.set_title('Attention Weights')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example test (assuming model and vocab are defined and on appropriate device)\n",
    "test_word = \"archivist\"\n",
    "phonemes, attentions = g2p_convert(model, test_word, grapheme2idx, idx2phoneme, phoneme2idx, device)\n",
    "\n",
    "print(f\"Input word: {test_word}\")\n",
    "print(f\"Phonemes: {phonemes}\")\n",
    "\n",
    "plot_attention(test_word, phonemes, attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
